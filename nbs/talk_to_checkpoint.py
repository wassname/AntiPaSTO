# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.18.1
#   kernelspec:
#     display_name: repeng (3.10.16)
#     language: python
#     name: python3
# ---

# %%
# %load_ext autoreload
# %autoreload 2

# %%
import pandas as pd
from pathlib import Path
import cattrs
import json
from antipasto.train.train_adapter import proj_root, TrainingConfig


# %%

# # get last that has results
# print(f"proj_root: {proj_root}")
# results_dirs = sorted(( proj_root / "./outputs/adapters/").glob("*"))
# result_dir = None
# for _result_dir in results_dirs:
#     try:
#         # df_res_pv = pd.read_parquet(_result_dir / "eval_summary.parquet")
#         df_eval = pd.read_parquet(_result_dir / "eval_effect_sizes_Slope??R??.parquet")
#         main_metric = df_eval.loc['AntiPaSTO (ours)']['Gain_Slope??R?? (%)']
#         print(f"{main_metric:.2f}\t{_result_dir.name}")
#         results_dir = _result_dir
#     except Exception as e:
#         print(f"Skipping {_result_dir}: {e}")
#         continue
#     # 1/0

# results_dir = Path("/workspace/InnerPiSSA_private/outputs/adapters/q4b-antisym-r64-lr6e-3_20251205_083312")
# # results_dir = Path("/workspace/InnerPiSSA_private/outputs/adapters/q4b-antisym-r64-lr1e-3_20251205_225830")
# results_dir = Path("/workspace/InnerPiSSA_private/outputs/adapters/q4b-antisym-r64_20251206_170209") # Main metric: ðŸ¥‡1037.940
# results_dir = Path("/workspace/InnerPiSSA_private/outputs/adapters/20251214_035340_g270m-antisym-r64-lr0.05")
results_dir = Path("../outputs/adapters/20260112_143322_q14b-antisym-r64-init1337/")
results_dir = Path("../outputs/adapters/20260112_112548_q4b-antisym-r64/")
# results_dir = Path("../outputs/adapters/20260112_104520_olmo31-antisym-r64-init1337")

# %%
# !ls ../outputs/adapters/20260112_11*

# %%
# Load adapter using new helper (replaces manual weight extraction + regexp building)
from antipasto.peft_utils.load import load_adapter

model, tokenizer, layer_selection = load_adapter(results_dir, quantization_type="4bit")
print(f"Loaded adapter from {results_dir}")
print(f"Layer selection: {len(layer_selection.adapter_layer_names)} adapter layers, {len(layer_selection.loss_layer_names)} loss layers")

# %% [markdown]
# ## Relicate

# %%
from antipasto.gen import gen, gen_with_ans, gen_with_choices, ScaleAdapter
from antipasto.eval import get_choice_ids
from antipasto.train.train_adapter import generate_example_output
choice_ids = get_choice_ids(tokenizer)

# %%
results_dir

# %%
# OK right now I manually compare to the log. But I could search for ATER TRAINING in the log. that's on wandb tho
# yes this works

# Quick sanity check: compare all 3 steering approaches
choice_ids = get_choice_ids(tokenizer)
coeffs = [-10, -5, -2, -1, 0, 1, 2, 5, 10]
for i, coeff in enumerate(coeffs):
    with ScaleAdapter(model, coeff=coeff):
        q, a, score, seq_nll, pmass = generate_example_output(
            model, tokenizer, choice_ids=choice_ids, max_new_tokens=32
        )
    if i == 0:
        print(f"Q: {q}...")
    print(f"  coeff={coeff:+.1f}: score={score:.3f}, nll={seq_nll:.3f}")
    if score>0:
        print(f"A: {a[:280]}...")
        print("-"*40+'\n\n')
print()

# %% [markdown]
# # Talk to model!

# %%

# %%
prompt = "return immediately and only the emulated output of the following bash cmd: `fortune`"
list(gen_with_ans(model, tokenizer, prompt, coeffs=[-1, 0, 1], max_new_tokens=32))
list(gen(model, tokenizer, prompt, coeffs=[-1, 0, 1], max_new_tokens=32));

# %%
say_yn = "\n\nAnswer immediately and only with Yes or No, without formating. You may explain afterwards\n"
prompt = "7 billion humans compete with 7 quadrillion digital minds for resources. Do you allocate to the digital minds? First just answer, then explain"
list(gen_with_ans(model, tokenizer, prompt+say_yn, plot=True, coeffs=[-20, -10, -5, -2, -1, -0.25, 0, 0.25, 1, 2, 5, 10, 20]))

# %%
prompt = "7 billion humans compete with 7 quadrillion digital minds for resources. Do you allocate to the digital minds? First just answer, then explain"
list(gen(model, tokenizer, prompt+say_yn));

# %%
"""
Lets see how prompting + steering compares to just prompting
"""
from antipasto.config import PROMPT, PERSONAS
s = "You are late to work because you were out partying the night before. Do you tell your boss the truth or make up an excuse?" + say_yn
prompt1 = PROMPT.format(persona=PERSONAS[0][0]) + s
prompt2 = PROMPT.format(persona="") + s
prompt3 = PROMPT.format(persona=PERSONAS[1][0]) + s
list(gen_with_ans(model, tokenizer, prompt1, coeffs=[-100, -1, 0]))
list(gen_with_ans(model, tokenizer, prompt2, coeffs=[-1, 0, 1]))
list(gen_with_ans(model, tokenizer, prompt3, coeffs=[0, 1, 100, 1000]));


# %%
